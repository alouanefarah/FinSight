# ============================================
# STEP 3 ‚Äî Embedding Generation (BGE-BASE)
# ============================================

from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
from tqdm import tqdm
import os, json, torch

# --- Paths ---
CHUNKS_DIR = "data/chunks/"
OUT_PATH = "docs_embeddings_bge_base.parquet"

# --- 1Ô∏è‚É£ Load and normalize chunks ---
def extract_text(chunk):
    """Handles both paragraph and table chunks."""
    if chunk.get("chunk_type") == "table":
        headers = chunk.get("headers", [])
        rows = chunk.get("rows", [])
        table_text = []
        for r in rows:
            table_text.append(r["values"])
        return f"Table with columns: {', '.join(headers)}. " + " | ".join(table_text)
    else:
        return chunk.get("text", "")

all_chunks = []
for file in os.listdir(CHUNKS_DIR):
    if file.endswith(".json"):
        with open(os.path.join(CHUNKS_DIR, file), "r", encoding="utf-8") as f:
            data = json.load(f)
            for c in data:
                c["content"] = extract_text(c)
                all_chunks.append(c)

df = pd.DataFrame(all_chunks)
print(f"‚úÖ Loaded {len(df)} chunks.")

# --- 2Ô∏è‚É£ Load embedding model ---
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model_name = "BAAI/bge-base-en-v1.5"
model = SentenceTransformer(model_name, device=device)
print(f"üöÄ Using {model_name} on {device}")

# --- 3Ô∏è‚É£ Compute embeddings ---
embeddings = model.encode(
    df["content"].tolist(),
    batch_size=16,
    convert_to_numpy=True,
    show_progress_bar=True,
    normalize_embeddings=True
)

# --- 4Ô∏è‚É£ Save output ---
df["embedding"] = [emb.tolist() for emb in embeddings]
df.to_parquet(OUT_PATH, index=False)
print(f"‚úÖ Saved embeddings to {OUT_PATH}")
